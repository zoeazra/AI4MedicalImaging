{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2021 University of Amsterdam\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to conditions.\n",
    "#\n",
    "# Author: Deep Learning Course (UvA) | Fall 2022 & Oliver Gurney-Champion | Spring 2023\n",
    "# Date modified: Jan 2023\n",
    "################################################################################\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Import requiered packages\n",
    "imports the packages and sets the random seed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "import helper_functions as hf\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# set random seed\n",
    "seed =42"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "login to your free wandb account. Note you will need to set up your account on https://wandb.ai/authorize\n",
    "wandb allows you to keep track of your neural network training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33moliverchampion\u001B[0m (\u001B[33mdl4mi\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simulate and view the IVIM data\n",
    "Here, we split our data into a training set, validation set and test set. Note that the current implementation only uses the training set and it is up to you (in your exercises) to also implement the validation and test run. At this point, we already split the data for you."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# set b-values at which we \"measure\" (i.e. simulate signal)\n",
    "bvalues=[0, 10, 20, 30, 50, 75, 100, 150, 300, 500, 700, 1000]\n",
    "\n",
    "## Set the random seeds for reproducibility\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "#determine\n",
    "batch_size = 128\n",
    "\n",
    "with wandb.init(project=\"AI_for_medical_imaging\", job_type=\"visualize data\") as run:\n",
    "    data_sim = hf.sim_signal(SNR=(10,40),bvalues=bvalues,sims=10000,seed=np.random.randint(1,10000))\n",
    "    # Only for visualisation purposes: here we create our \"Artifact\" in wandb --> this allows viewing the data in your wandb account\n",
    "    for i in range(4):\n",
    "        #make b-value data pairs\n",
    "        example_data=[[x,y] for (x,y) in zip(bvalues,data_sim[0][i])]\n",
    "        # put it in a table\n",
    "        table = wandb.Table(data=example_data, columns=[\"b-values\", \"signal\"])\n",
    "        #tell wandb to plot the table\n",
    "        wandb.log({\"data_plot \" + str(i): wandb.plot.scatter(table, \"b-values\", \"signal\")})\n",
    "\n",
    "    # here we split the data into train (70%), test (15%) and validation (15%) sets\n",
    "    #split = int(np.floor(len(data_sim[0]) * 0.7))\n",
    "    train_set, test_set, val_set = torch.utils.data.random_split([[data_sim[0][i,:],data_sim[1][i],data_sim[2][i],data_sim[3][i]] for i in range(len(data_sim[3]))],[0.7,0.15,0.15])\n",
    "    #split = int(np.floor(len(rest) * 0.5))\n",
    "    #test_set, val_set = torch.utils.data.random_split([[rest[0][i,:],rest[1][i],rest[2][i],rest[3][i]] for i in range(len(rest[3]))],[split, len(rest[0]) - split])\n",
    "\n",
    "    # train loader loads the trianing data. We want to shuffle to make sure data order is modified each epoch and different data is selected each epoch.\n",
    "    trainloader = torch.utils.data.DataLoader(train_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   drop_last=True)\n",
    "    # validation data is loaded here. By not shuffling, we make sure the same data is loaded for validation every time. We can use substantially more data per batch as we are not training.\n",
    "    inferloader = torch.utils.data.DataLoader(val_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=False,\n",
    "                                   drop_last=True)\n",
    "        # validation data is loaded here. By not shuffling, we make sure the same data is loaded for validation every time. We can use substantially more data per batch as we are not training.\n",
    "    testloader = torch.utils.data.DataLoader(test_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=False,\n",
    "                                   drop_last=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3B: Design a neural network\n",
    "Combine all modules into a multilayer perceptron (MLP) in “exercise_3.ipynb”. Use ReLU activation functions after each fully connected linear module, except for the last module, after which a Tanh module should be used (constraining parameters between [-1, 1]). The number of layers and number of nodes per layer should be adjustable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module implements a multi-layer perceptron (MLP) in NumPy.\n",
    "You should fill in code into indicated sections.\n",
    "\"\"\"\n",
    "from modules import MLP #you need to adapt these\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Exercise 3C-D\n",
    "C.\tTrain the neural network using your code in “exercise_3.ipynb”. As a sanity check, a network with 2 layers (64, 32), combined with a lr of 0.001, 30 epochs, and a batch size of 128 should work reasonably well and result in a systematic error of around 0.2% and a random error of around 20%.\n",
    "D.\tIt now uses the mini-batch stochastic gradient descent algorithm. Add momentum to the training update of the weights.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define parameters\n",
    "hidden_layers=(64,32)\n",
    "model=MLP(len(bvalues), hidden_layers, 1)\n",
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "# initialize wandb\n",
    "wandb.init(\n",
    "        project=\"AI_for_medical_imaging\", job_type=\"training\")\n",
    "\n",
    "loss_module1 = MSE()\n",
    "\n",
    "# set random seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# probe available devices\n",
    "if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# loop over epochs\n",
    "for epoch in range(epochs):\n",
    "    # initiate losses to 0\n",
    "    train_loss_f = 0\n",
    "    val_loss_f = 0\n",
    "    #loop over all training data\n",
    "    for x in trainloader:\n",
    "        # get data (x[0]) and put the data on the GPU if available\n",
    "        batch=x[0].numpy()\n",
    "        # get the reference f (x[2]) --> note x[1] and x[3] are D and Dp respectively\n",
    "        f_ref = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        # put the data through the neural network\n",
    "        f_pred = model.forward(batch)\n",
    "        # calculate loss (compare predicted f to the ground trueth)\n",
    "        train_loss_f += np.mean(loss_module1.forward(f_pred, f_ref))\n",
    "        # propogate the loss through the network (calculate d_wights/d_loss)\n",
    "        model.backward(loss_module1.backward())\n",
    "\n",
    "        # update all weights accoording to their derrivatives to the loss.\n",
    "        for objs in model.LM:\n",
    "            objs.bias = objs.bias - np.transpose([learning_rate * objs.grads['bias']])\n",
    "            objs.weights = objs.weights - learning_rate * objs.grads['weight']\n",
    "        model.clear_cache()\n",
    "    # initialize error_metrics\n",
    "    SD_val=0\n",
    "    sys_val=0\n",
    "    wandb.log({\"loss/train\": train_loss_f/trainloader.__len__(),\"loss/val\": val_loss_f/inferloader.__len__(),\"error/random error\":SD_val,\"error/systematic error\":sys_val})\n",
    "    print('epoch = ' + str(epoch) + ' train loss =' + str(train_loss_f/trainloader.__len__()) +' val loss =' + str(val_loss_f/inferloader.__len__()) + 'the systematic error is ' + str(sys_val) + ' and the random error is ' + str(SD_val))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
