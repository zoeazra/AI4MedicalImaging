{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2021 University of Amsterdam\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to conditions.\n",
    "#\n",
    "# Author: Deep Learning Course (UvA) | Fall 2022 & Oliver Gurney-Champion | Spring 2023\n",
    "# Date modified: Jan 2023\n",
    "################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import requiered packages\n",
    "imports the packages and sets the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "import helper_functions as hf\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# set random seed\n",
    "seed =42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "login to your free wandb account. Note you will need to set up your account on https://wandb.ai/authorize\n",
    "wandb allows you to keep track of your neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33melenaliarou26\u001b[0m (\u001b[33melenaliarou26-vrije-universiteit-amsterdam\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Simulate and view the IVIM data\n",
    "Here, we split our data into a training set, validation set and test set. Note that the current implementation only uses the training set and it is up to you (in your exercises) to also implement the validation and test run. At this point, we already split the data for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/elenaliarou/Documents/master/block4/dl/AI4MedicalImaging/Assignment 1/wandb/run-20250217_121249-fiyn0p86</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging/runs/fiyn0p86' target=\"_blank\">robust-glade-571</a></strong> to <a href='https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging' target=\"_blank\">https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging/runs/fiyn0p86' target=\"_blank\">https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging/runs/fiyn0p86</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">robust-glade-571</strong> at: <a href='https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging/runs/fiyn0p86' target=\"_blank\">https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging/runs/fiyn0p86</a><br> View project at: <a href='https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging' target=\"_blank\">https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging</a><br>Synced 5 W&B file(s), 4 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250217_121249-fiyn0p86/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set b-values at which we \"measure\" (i.e. simulate signal)\n",
    "bvalues=[0, 10, 20, 30, 50, 75, 100, 150, 300, 500, 700, 1000]\n",
    "\n",
    "## Set the random seeds for reproducibility\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "#determine\n",
    "batch_size = 128\n",
    "\n",
    "with wandb.init(project=\"AI_for_medical_imaging\", job_type=\"visualize data\") as run:\n",
    "    data_sim = hf.sim_signal(SNR=(10,40),bvalues=bvalues,sims=10000,seed=np.random.randint(1,10000))\n",
    "    # Only for visualisation purposes: here we create our \"Artifact\" in wandb --> this allows viewing the data in your wandb account\n",
    "    for i in range(4):\n",
    "        #make b-value data pairs\n",
    "        example_data=[[x,y] for (x,y) in zip(bvalues,data_sim[0][i])]\n",
    "        # put it in a table\n",
    "        table = wandb.Table(data=example_data, columns=[\"b-values\", \"signal\"])\n",
    "        #tell wandb to plot the table\n",
    "        wandb.log({\"data_plot \" + str(i): wandb.plot.scatter(table, \"b-values\", \"signal\")})\n",
    "\n",
    "    # here we split the data into train (70%), test (15%) and validation (15%) sets\n",
    "    #split = int(np.floor(len(data_sim[0]) * 0.7))\n",
    "    train_set, test_set, val_set = torch.utils.data.random_split([[data_sim[0][i,:],data_sim[1][i],data_sim[2][i],data_sim[3][i]] for i in range(len(data_sim[3]))],[0.7,0.15,0.15])\n",
    "    #split = int(np.floor(len(rest) * 0.5))\n",
    "    #test_set, val_set = torch.utils.data.random_split([[rest[0][i,:],rest[1][i],rest[2][i],rest[3][i]] for i in range(len(rest[3]))],[split, len(rest[0]) - split])\n",
    "\n",
    "    # train loader loads the trianing data. We want to shuffle to make sure data order is modified each epoch and different data is selected each epoch.\n",
    "    trainloader = torch.utils.data.DataLoader(train_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   drop_last=True)\n",
    "    # validation data is loaded here. By not shuffling, we make sure the same data is loaded for validation every time. We can use substantially more data per batch as we are not training.\n",
    "    inferloader = torch.utils.data.DataLoader(val_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=False,\n",
    "                                   drop_last=True)\n",
    "        # validation data is loaded here. By not shuffling, we make sure the same data is loaded for validation every time. We can use substantially more data per batch as we are not training.\n",
    "    testloader = torch.utils.data.DataLoader(test_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=False,\n",
    "                                   drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3 A.\tProgram the different neural network modules (Lineairemodule, Relumodule, Tanhmodule, MSE) in “modules.py”. Use “do_unittest.py” to test whether the forward and backward passes are correct. Make only use of matrix multiplications, no for loops. Points will be deduced when for-loops are used where matrix multiplications were possible.\n",
    "\n",
    "3 B.  Combine all modules into a multilayer perceptron (MLP) in “exercise_3.ipynb”. Use ReLU activation functions after each fully connected linear module, except for the last module, after which a Tanh module should be used (constraining parameters between [-1, 1]). The number of layers and number of nodes per layer should be adjustable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module implements a multi-layer perceptron (MLP) in NumPy.\n",
    "You should fill in code into indicated sections.\n",
    "\"\"\"\n",
    "from modules import MLP #you need to adapt these\n",
    "from modules import MSE \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 3C-D\n",
    "C.\tTrain the neural network using just python and numpy code. You can find modules in \"modules.py\" which you will first have to fill in and train. You can then use these modules here!\n",
    "\n",
    "You can use \"do_unittest.py\" to test whether the modules from \"modules.py\" have been implemented correctly.\n",
    "\n",
    "You will need to add the updating of the gradients. As a sanity check, a network with 2 layers (64, 32), combined with a lr of 0.001, 30 epochs, and a batch size of 128 should work reasonably well and result in a systematic error of around 0.2% and a random error of around 20%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/elenaliarou/Documents/master/block4/dl/AI4MedicalImaging/Assignment 1/wandb/run-20250217_121329-ai1m14pz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging/runs/ai1m14pz' target=\"_blank\">fearless-durian-572</a></strong> to <a href='https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging' target=\"_blank\">https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging/runs/ai1m14pz' target=\"_blank\">https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging/runs/ai1m14pz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 train loss =0.15423788384538423 val loss =0.1271301625248625the systematic error is -0.29600267691634125 and the random error is 0.19854888033706963\n",
      "epoch = 1 train loss =0.13217897239096899 val loss =0.10871370025406266the systematic error is -0.2630571181937608 and the random error is 0.19855409867653254\n",
      "epoch = 2 train loss =0.11437143918671744 val loss =0.09415864856279864the systematic error is -0.2337564878934618 and the random error is 0.19855887090847502\n",
      "epoch = 3 train loss =0.09979292400944273 val loss =0.08270093795654378the systematic error is -0.20780544078203178 and the random error is 0.19856209764840882\n",
      "epoch = 4 train loss =0.08879965592526665 val loss =0.07363429080085858the systematic error is -0.1847039387503761 and the random error is 0.19856438130295143\n",
      "epoch = 5 train loss =0.07966130673729477 val loss =0.06648139481466946the systematic error is -0.16420099079996783 and the random error is 0.19856609825792582\n",
      "epoch = 6 train loss =0.07238069417046097 val loss =0.06081678335760507the systematic error is -0.14593432581813362 and the random error is 0.19856742071741765\n",
      "epoch = 7 train loss =0.0666247695281834 val loss =0.056336574426166214the systematic error is -0.12967733203044876 and the random error is 0.198568445667376\n",
      "epoch = 8 train loss =0.06193828301299694 val loss =0.052785253481612116the systematic error is -0.11517195656703558 and the random error is 0.1985692236629624\n",
      "epoch = 9 train loss =0.05815194281990349 val loss =0.049956615902553535the systematic error is -0.10215537274599411 and the random error is 0.19856978053972293\n",
      "epoch = 10 train loss =0.05515647524856121 val loss =0.04771396839779633the systematic error is -0.09051484819690057 and the random error is 0.19857011905636815\n",
      "epoch = 11 train loss =0.052547532041512884 val loss =0.045934019788415756the systematic error is -0.08008085555329975 and the random error is 0.1985702330618365\n",
      "epoch = 12 train loss =0.05045471115413599 val loss =0.044524393702080216the systematic error is -0.07073445915977773 and the random error is 0.19857011553209516\n",
      "epoch = 13 train loss =0.04869352125925343 val loss =0.043409468266787155the systematic error is -0.06235849753415476 and the random error is 0.1985697531780613\n",
      "epoch = 14 train loss =0.047387021951384695 val loss =0.042520925231144945the systematic error is -0.05477496720655303 and the random error is 0.1985691059079815\n",
      "epoch = 15 train loss =0.04626867130246319 val loss =0.041818326946131225the systematic error is -0.04793790617788563 and the random error is 0.19856824139679322\n",
      "epoch = 16 train loss =0.04549762615672982 val loss =0.041257525832175526the systematic error is -0.04168452868556735 and the random error is 0.19856734025850462\n",
      "epoch = 17 train loss =0.04470190676765148 val loss =0.040820352858876the systematic error is -0.036066082909192304 and the random error is 0.1985664958807587\n",
      "epoch = 18 train loss =0.04402999469168532 val loss =0.04047596472239064the systematic error is -0.03092985754061004 and the random error is 0.1985657855486557\n",
      "epoch = 19 train loss =0.043592188508921645 val loss =0.04020983283280482the systematic error is -0.026282971175926182 and the random error is 0.19856510432758942\n",
      "epoch = 20 train loss =0.04312920517528391 val loss =0.040007558681028496the systematic error is -0.02210817564812186 and the random error is 0.1985644716640461\n",
      "epoch = 21 train loss =0.04284739284309493 val loss =0.039853300782123534the systematic error is -0.01829665979854312 and the random error is 0.19856383079441212\n",
      "epoch = 22 train loss =0.04228480092975558 val loss =0.03973965151844328the systematic error is -0.014878971017976235 and the random error is 0.19856316175955446\n",
      "epoch = 23 train loss =0.0421586223471024 val loss =0.03965627365388134the systematic error is -0.01175967022214834 and the random error is 0.19856244722207606\n",
      "epoch = 24 train loss =0.042054149253245444 val loss =0.03959651151248206the systematic error is -0.008878356159642956 and the random error is 0.19856169817468128\n",
      "epoch = 25 train loss =0.04182015080455596 val loss =0.039556757655621284the systematic error is -0.006272656994627048 and the random error is 0.1985610058100639\n",
      "epoch = 26 train loss =0.04171063949793675 val loss =0.039532140839357445the systematic error is -0.0038719515156508777 and the random error is 0.1985603432761954\n",
      "epoch = 27 train loss =0.0415479320704596 val loss =0.03951969620583219the systematic error is -0.0016806831949334447 and the random error is 0.1985596440295257\n",
      "epoch = 28 train loss =0.04139770606413393 val loss =0.0395167192628129the systematic error is 0.00034060584150788796 and the random error is 0.1985589683111445\n",
      "epoch = 29 train loss =0.041296747342885784 val loss =0.039521050446760075the systematic error is 0.002173829922955003 and the random error is 0.1985582666167991\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "hidden_layers=(64,32)\n",
    "model=MLP(len(bvalues), hidden_layers, 1)\n",
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "# initialize wandb\n",
    "wandb.init(\n",
    "        project=\"AI_for_medical_imaging\", job_type=\"training\")\n",
    "\n",
    "loss_module1 = MSE()\n",
    "\n",
    "# set random seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# probe available devices\n",
    "if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# loop over epochs\n",
    "for epoch in range(epochs):\n",
    "    # initiate losses to 0\n",
    "    train_loss_f = 0\n",
    "    val_loss_f = 0\n",
    "    #loop over all training data\n",
    "    for x in trainloader:\n",
    "        # get data (x[0]) and put the data on the GPU if available\n",
    "        batch=x[0].numpy()\n",
    "        # get the reference f (x[2]) --> note x[1] and x[3] are D and Dp respectively\n",
    "        f_ref = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        # put the data through the neural network\n",
    "        f_pred = model.forward(batch)\n",
    "        # calculate loss (compare predicted f to the ground trueth)\n",
    "        train_loss_f += np.mean(loss_module1.forward(f_pred, f_ref))\n",
    "        # propogate the loss through the network (calculate d_wights/d_loss)\n",
    "        model.backward(loss_module1.backward())\n",
    "\n",
    "        #######################\n",
    "        # PUT YOUR CODE HERE  #\n",
    "        #######################\n",
    "        #updating of the gradients\n",
    "        for layer in model.LM: \n",
    "            layer.weights -= learning_rate * layer.grads['weight']\n",
    "            layer.bias -= learning_rate * layer.grads['bias']\n",
    "\n",
    "        #######################\n",
    "        # END OF YOUR CODE    #\n",
    "        #######################\n",
    "        \n",
    "    # initialize error_metrics\n",
    "    SD_val=0\n",
    "    sys_val=0\n",
    "    for x in inferloader:\n",
    "        batch=x[0].numpy()\n",
    "        f_ref_val = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        f_pred_val = model.forward(batch)\n",
    "        val_loss_f += np.mean(loss_module1.forward(f_pred_val, f_ref_val))\n",
    "        SD, sys = hf.error_metrics(f_pred_val,f_ref_val)\n",
    "        SD_val += SD**2\n",
    "        sys_val += sys\n",
    "    SD_val = np.sqrt(SD_val/inferloader.__len__())\n",
    "    sys_val = sys_val/inferloader.__len__()\n",
    "\n",
    "    model.clear_cache()\n",
    "    \n",
    "    wandb.log({\"loss/train\": train_loss_f/trainloader.__len__(),\"loss/val\": val_loss_f/inferloader.__len__(),\"error/random error\":SD_val,\"error/systematic error\":sys_val})\n",
    "    print('epoch = ' + str(epoch) + ' train loss =' + str(train_loss_f/trainloader.__len__()) +' val loss =' + str(val_loss_f/inferloader.__len__()) + 'the systematic error is ' + str(sys_val) + ' and the random error is ' + str(SD_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.\tIt now uses the mini-batch stochastic gradient descent algorithm. Add momentum to the training update of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
