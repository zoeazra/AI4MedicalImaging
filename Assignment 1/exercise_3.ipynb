{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2021 University of Amsterdam\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to conditions.\n",
    "#\n",
    "# Author: Deep Learning Course (UvA) | Fall 2022 & Oliver Gurney-Champion | Spring 2023\n",
    "# Date modified: Jan 2023\n",
    "################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import requiered packages\n",
    "imports the packages and sets the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "import helper_functions as hf\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# set random seed\n",
    "seed =42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "login to your free wandb account. Note you will need to set up your account on https://wandb.ai/authorize\n",
    "wandb allows you to keep track of your neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Simulate and view the IVIM data\n",
    "Here, we split our data into a training set, validation set and test set. Note that the current implementation only uses the training set and it is up to you (in your exercises) to also implement the validation and test run. At this point, we already split the data for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>error/random error</td><td>███████████████▃▁▁█████▇▆▁▁█████████████</td></tr><tr><td>error/systematic error</td><td>▁▅▇▇██████████▇██▇▇▇████▇▇█▇▇▇██████▁▅▆▇</td></tr><tr><td>loss/train</td><td>█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▁▃▃▃▃▃▃▃▃▁▁█▄▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>loss/val</td><td>█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▄▃▁▁▅▅▅▅▅▅▅▅▃▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>error/random error</td><td>0.19508</td></tr><tr><td>error/systematic error</td><td>-0.00953</td></tr><tr><td>loss/train</td><td>0.04119</td></tr><tr><td>loss/val</td><td>0.04018</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wild-tree-194</strong> at: <a href='https://wandb.ai/zoeazra-vu/AI_for_medical_imaging/runs/y2m6orzc' target=\"_blank\">https://wandb.ai/zoeazra-vu/AI_for_medical_imaging/runs/y2m6orzc</a><br> View project at: <a href='https://wandb.ai/zoeazra-vu/AI_for_medical_imaging' target=\"_blank\">https://wandb.ai/zoeazra-vu/AI_for_medical_imaging</a><br>Synced 5 W&B file(s), 4 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250218_162151-y2m6orzc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set b-values at which we \"measure\" (i.e. simulate signal)\n",
    "bvalues=[0, 10, 20, 30, 50, 75, 100, 150, 300, 500, 700, 1000]\n",
    "\n",
    "## Set the random seeds for reproducibility\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "#determine\n",
    "batch_size = 128\n",
    "\n",
    "with wandb.init(project=\"AI_for_medical_imaging\", job_type=\"visualize data\") as run:\n",
    "    data_sim = hf.sim_signal(SNR=(10,40),bvalues=bvalues,sims=10000,seed=np.random.randint(1,10000))\n",
    "    # Only for visualisation purposes: here we create our \"Artifact\" in wandb --> this allows viewing the data in your wandb account\n",
    "    for i in range(4):\n",
    "        #make b-value data pairs\n",
    "        example_data=[[x,y] for (x,y) in zip(bvalues,data_sim[0][i])]\n",
    "        # put it in a table\n",
    "        table = wandb.Table(data=example_data, columns=[\"b-values\", \"signal\"])\n",
    "        #tell wandb to plot the table\n",
    "        wandb.log({\"data_plot \" + str(i): wandb.plot.scatter(table, \"b-values\", \"signal\")})\n",
    "\n",
    "    # here we split the data into train (70%), test (15%) and validation (15%) sets\n",
    "    #split = int(np.floor(len(data_sim[0]) * 0.7))\n",
    "    train_set, test_set, val_set = torch.utils.data.random_split([[data_sim[0][i,:],data_sim[1][i],data_sim[2][i],data_sim[3][i]] for i in range(len(data_sim[3]))],[0.7,0.15,0.15])\n",
    "    #split = int(np.floor(len(rest) * 0.5))\n",
    "    #test_set, val_set = torch.utils.data.random_split([[rest[0][i,:],rest[1][i],rest[2][i],rest[3][i]] for i in range(len(rest[3]))],[split, len(rest[0]) - split])\n",
    "\n",
    "    # train loader loads the trianing data. We want to shuffle to make sure data order is modified each epoch and different data is selected each epoch.\n",
    "    trainloader = torch.utils.data.DataLoader(train_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   drop_last=True)\n",
    "    # validation data is loaded here. By not shuffling, we make sure the same data is loaded for validation every time. We can use substantially more data per batch as we are not training.\n",
    "    inferloader = torch.utils.data.DataLoader(val_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=False,\n",
    "                                   drop_last=True)\n",
    "        # validation data is loaded here. By not shuffling, we make sure the same data is loaded for validation every time. We can use substantially more data per batch as we are not training.\n",
    "    testloader = torch.utils.data.DataLoader(test_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=False,\n",
    "                                   drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3 A.\tProgram the different neural network modules (Lineairemodule, Relumodule, Tanhmodule, MSE) in “modules.py”. Use “do_unittest.py” to test whether the forward and backward passes are correct. Make only use of matrix multiplications, no for loops. Points will be deduced when for-loops are used where matrix multiplications were possible.\n",
    "\n",
    "3 B.  Combine all modules into a multilayer perceptron (MLP) in “exercise_3.ipynb”. Use ReLU activation functions after each fully connected linear module, except for the last module, after which a Tanh module should be used (constraining parameters between [-1, 1]). The number of layers and number of nodes per layer should be adjustable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module implements a multi-layer perceptron (MLP) in NumPy.\n",
    "You should fill in code into indicated sections.\n",
    "\"\"\"\n",
    "from modules import MLP #you need to adapt these\n",
    "from modules import MSE \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 3C-D\n",
    "C.\tTrain the neural network using just python and numpy code. You can find modules in \"modules.py\" which you will first have to fill in and train. You can then use these modules here!\n",
    "\n",
    "You can use \"do_unittest.py\" to test whether the modules from \"modules.py\" have been implemented correctly.\n",
    "\n",
    "You will need to add the updating of the gradients. As a sanity check, a network with 2 layers (64, 32), combined with a lr of 0.001, 30 epochs, and a batch size of 128 should work reasonably well and result in a systematic error of around 0.2% and a random error of around 20%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 train loss =0.15110451695738725 val loss =0.13967357859094745the systematic error is -0.31718562974888537 and the random error is 0.19676503107902998\n",
      "epoch = 1 train loss =0.12933362420408548 val loss =0.12025642650770227the systematic error is -0.2849268648928854 and the random error is 0.19678077472318645\n",
      "epoch = 2 train loss =0.1121316604476833 val loss =0.10468654363875869the systematic error is -0.2561475779295251 and the random error is 0.19678545903920672\n",
      "epoch = 3 train loss =0.0983904405573263 val loss =0.09221528707486172the systematic error is -0.2305176897293819 and the random error is 0.19679031049906748\n",
      "epoch = 4 train loss =0.08737897447624292 val loss =0.08222906062450137the systematic error is -0.20772855253327566 and the random error is 0.19679290461767548\n",
      "epoch = 5 train loss =0.07861356029572032 val loss =0.07422724175725935the systematic error is -0.18748024761041301 and the random error is 0.19679413857179398\n",
      "epoch = 6 train loss =0.07170710960158146 val loss =0.06778347331528328the systematic error is -0.16942532017882647 and the random error is 0.19679447849489262\n",
      "epoch = 7 train loss =0.06581387427468977 val loss =0.06260646166661951the systematic error is -0.15338867699886594 and the random error is 0.19679407397006882\n",
      "epoch = 8 train loss =0.06138803825595004 val loss =0.05840881358523697the systematic error is -0.13903542259539442 and the random error is 0.1967930323210873\n",
      "epoch = 9 train loss =0.057586462446808144 val loss =0.05501130837372421the systematic error is -0.12622984815845195 and the random error is 0.19679143274771982\n",
      "epoch = 10 train loss =0.054605440940893306 val loss =0.05224432273597805the systematic error is -0.11475095567640702 and the random error is 0.1967894266565915\n",
      "epoch = 11 train loss =0.05217562355425039 val loss =0.04998700083187842the systematic error is -0.10445774260560257 and the random error is 0.19678700260598078\n",
      "epoch = 12 train loss =0.050111273493543926 val loss =0.04814338858658076the systematic error is -0.09523056058256073 and the random error is 0.19678435253621387\n",
      "epoch = 13 train loss =0.04855058789167827 val loss =0.046625547825631the systematic error is -0.08690293333606541 and the random error is 0.19678157931761864\n",
      "epoch = 14 train loss =0.04727669215338078 val loss =0.045369942060458135the systematic error is -0.07935735493534991 and the random error is 0.19677887008136127\n",
      "epoch = 15 train loss =0.04619149190281108 val loss =0.04433352260452543the systematic error is -0.0725409115993644 and the random error is 0.1967763152408193\n",
      "epoch = 16 train loss =0.045275970129757204 val loss =0.04348283122008923the systematic error is -0.06642571058663506 and the random error is 0.1967740895163471\n",
      "epoch = 17 train loss =0.04443145649006031 val loss =0.04277576678193154the systematic error is -0.060877279371788724 and the random error is 0.19677224231418103\n",
      "epoch = 18 train loss =0.0439533644804646 val loss =0.042187833398959235the systematic error is -0.05584565711724483 and the random error is 0.196770658403443\n",
      "epoch = 19 train loss =0.04339322646882296 val loss =0.04169638485886352the systematic error is -0.05126201980012917 and the random error is 0.19676938013661502\n",
      "epoch = 20 train loss =0.042981960199691104 val loss =0.04128382846614959the systematic error is -0.04707091550771954 and the random error is 0.1967682875182945\n",
      "epoch = 21 train loss =0.04262154801536643 val loss =0.040944725911076105the systematic error is -0.043324103603039635 and the random error is 0.19676725356936917\n",
      "epoch = 22 train loss =0.04231177290159889 val loss =0.040656080166604186the systematic error is -0.039857913846505104 and the random error is 0.19676644258171963\n",
      "epoch = 23 train loss =0.042089539098154646 val loss =0.04041701717520384the systematic error is -0.0367410923746681 and the random error is 0.1967656402586933\n",
      "epoch = 24 train loss =0.04197917265766857 val loss =0.04021672149918776the systematic error is -0.0339100833887666 and the random error is 0.19676492960846403\n",
      "epoch = 25 train loss =0.04174998659152815 val loss =0.04004608031191605the systematic error is -0.031296956050699425 and the random error is 0.19676430669060094\n",
      "epoch = 26 train loss =0.04152693430600567 val loss =0.03990443201643992the systematic error is -0.028949794651946133 and the random error is 0.19676370236525131\n",
      "epoch = 27 train loss =0.04145370346041313 val loss =0.039781945107942515the systematic error is -0.026754766925938 and the random error is 0.1967631621355263\n",
      "epoch = 28 train loss =0.04133593247398752 val loss =0.03967971498394247the systematic error is -0.024774826560816223 and the random error is 0.19676264523311174\n",
      "epoch = 29 train loss =0.04121224413557597 val loss =0.039592056671448675the systematic error is -0.022941813226515763 and the random error is 0.1967621581594563\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "hidden_layers=(64,32)\n",
    "model=MLP(len(bvalues), hidden_layers, 1)\n",
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "# initialize wandb\n",
    "wandb.init(\n",
    "        project=\"AI_for_medical_imaging\", job_type=\"training\")\n",
    "\n",
    "loss_module1 = MSE()\n",
    "\n",
    "# set random seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# probe available devices\n",
    "if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# loop over epochs\n",
    "for epoch in range(epochs):\n",
    "    # initiate losses to 0\n",
    "    train_loss_f = 0\n",
    "    val_loss_f = 0\n",
    "    #loop over all training data\n",
    "    for x in trainloader:\n",
    "        # get data (x[0]) and put the data on the GPU if available\n",
    "        batch=x[0].numpy()\n",
    "        # get the reference f (x[2]) --> note x[1] and x[3] are D and Dp respectively\n",
    "        f_ref = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        # put the data through the neural network\n",
    "        f_pred = model.forward(batch)\n",
    "        # calculate loss (compare predicted f to the ground trueth)\n",
    "        train_loss_f += np.mean(loss_module1.forward(f_pred, f_ref))\n",
    "        # propogate the loss through the network (calculate d_wights/d_loss)\n",
    "        model.backward(loss_module1.backward())\n",
    "\n",
    "        #######################\n",
    "        # PUT YOUR CODE HERE  #\n",
    "        #######################\n",
    "        #updating of the gradients\n",
    "        \n",
    "        for layer in model.LM: \n",
    "            layer.weights -= learning_rate * layer.grads['weight']\n",
    "            layer.bias -= learning_rate * layer.grads['bias']\n",
    "\n",
    "        #######################\n",
    "        # END OF YOUR CODE    #\n",
    "        #######################\n",
    "        \n",
    "    # initialize error_metrics\n",
    "    SD_val=0\n",
    "    sys_val=0\n",
    "    for x in inferloader:\n",
    "        batch=x[0].numpy()\n",
    "        f_ref_val = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        f_pred_val = model.forward(batch)\n",
    "        val_loss_f += np.mean(loss_module1.forward(f_pred_val, f_ref_val))\n",
    "        SD, sys = hf.error_metrics(f_pred_val,f_ref_val)\n",
    "        SD_val += SD**2\n",
    "        sys_val += sys\n",
    "    SD_val = np.sqrt(SD_val/inferloader.__len__())\n",
    "    sys_val = sys_val/inferloader.__len__()\n",
    "\n",
    "    model.clear_cache()\n",
    "    \n",
    "    wandb.log({\"loss/train\": train_loss_f/trainloader.__len__(),\"loss/val\": val_loss_f/inferloader.__len__(),\"error/random error\":SD_val,\"error/systematic error\":sys_val})\n",
    "    print('epoch = ' + str(epoch) + ' train loss =' + str(train_loss_f/trainloader.__len__()) +' val loss =' + str(val_loss_f/inferloader.__len__()) + 'the systematic error is ' + str(sys_val) + ' and the random error is ' + str(SD_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.\tIt now uses the mini-batch stochastic gradient descent algorithm. Add momentum to the training update of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 train loss =0.10312214855702172 val loss =0.05560625150320084the systematic error is -0.12854279880753952 and the random error is 0.19680577501333985\n",
      "epoch = 1 train loss =0.046750459772518065 val loss =0.04078000140690448the systematic error is -0.04121112762335953 and the random error is 0.19680234953663206\n",
      "epoch = 2 train loss =0.041431747821242135 val loss =0.039372700060214934the systematic error is -0.017150636223985036 and the random error is 0.19679457852070997\n",
      "epoch = 3 train loss =0.04083683882174035 val loss =0.03914080348080447the systematic error is -0.00793795470707066 and the random error is 0.19679265411365188\n",
      "epoch = 4 train loss =0.040745308635939205 val loss =0.03910012618253933the systematic error is -0.00482494276550008 and the random error is 0.19679027625426668\n",
      "epoch = 5 train loss =0.04085400873604819 val loss =0.03908567556208269the systematic error is -0.003515116936150102 and the random error is 0.1967813888917374\n",
      "epoch = 6 train loss =0.040911618807435476 val loss =0.03908482697006589the systematic error is -0.003540723444390855 and the random error is 0.19677879899631018\n",
      "epoch = 7 train loss =0.040876023738989305 val loss =0.03908240885342942the systematic error is -0.003315067531773042 and the random error is 0.19677660170242692\n",
      "epoch = 8 train loss =0.04084998481560338 val loss =0.03907879267440275the systematic error is -0.002870134818148949 and the random error is 0.19677442155924513\n",
      "epoch = 9 train loss =0.040848210662789576 val loss =0.03907911998519817the systematic error is -0.0030735594263743863 and the random error is 0.19677219691501793\n",
      "epoch = 10 train loss =0.04087466392185478 val loss =0.03908233628474167the systematic error is -0.0036835239751094716 and the random error is 0.19676991293311302\n",
      "epoch = 11 train loss =0.040857811850630636 val loss =0.03907671850378174the systematic error is -0.0029685787026015927 and the random error is 0.19676773883421353\n",
      "epoch = 12 train loss =0.04084920447558115 val loss =0.03907998208306979the systematic error is -0.003601090397076449 and the random error is 0.19676548875391311\n",
      "epoch = 13 train loss =0.040852808327471436 val loss =0.039077071918940394the systematic error is -0.003302363167707372 and the random error is 0.19676334916347496\n",
      "epoch = 14 train loss =0.040827950226267624 val loss =0.039072128401079625the systematic error is -0.0026072965854963682 and the random error is 0.19676124056071267\n",
      "epoch = 15 train loss =0.040836986438843474 val loss =0.03907209711932981the systematic error is -0.002756445839245093 and the random error is 0.19675914299769479\n",
      "epoch = 16 train loss =0.040907362961056445 val loss =0.03907760424287377the systematic error is -0.0037358161460244806 and the random error is 0.1967569956940372\n",
      "epoch = 17 train loss =0.04077211069220376 val loss =0.039074914035717785the systematic error is -0.0034733340367969086 and the random error is 0.19675498261314173\n",
      "epoch = 18 train loss =0.040902640320090555 val loss =0.03907128147792174the systematic error is -0.0030378710426206216 and the random error is 0.19675297132612987\n",
      "epoch = 19 train loss =0.04084201232529653 val loss =0.039074792489534396the systematic error is -0.0036819995116716725 and the random error is 0.19675090892890837\n",
      "epoch = 20 train loss =0.040815800953671696 val loss =0.03906800011136733the systematic error is -0.002745238204765826 and the random error is 0.19674896188596766\n",
      "epoch = 21 train loss =0.04086064557510057 val loss =0.0390681950086827the systematic error is -0.0029203411655625896 and the random error is 0.19674695071399045\n",
      "epoch = 22 train loss =0.04080044237666188 val loss =0.03907189393656588the systematic error is -0.00361205184682689 and the random error is 0.1967448829260279\n",
      "epoch = 23 train loss =0.040845387257716845 val loss =0.03906538043402224the systematic error is -0.0027651847437936273 and the random error is 0.19674207474818972\n",
      "epoch = 24 train loss =0.04094398059283885 val loss =0.039070767486013284the systematic error is -0.0038310310023792343 and the random error is 0.1967379336407114\n",
      "epoch = 25 train loss =0.040860599387610094 val loss =0.03906536883408452the systematic error is -0.0031831750814280742 and the random error is 0.19673577802983921\n",
      "epoch = 26 train loss =0.0408012524937932 val loss =0.03906806936179899the systematic error is -0.003706509331181318 and the random error is 0.196733494767394\n",
      "epoch = 27 train loss =0.04081468278494488 val loss =0.039062192648417524the systematic error is -0.00295212469236472 and the random error is 0.19673134148964921\n",
      "epoch = 28 train loss =0.040811766454961665 val loss =0.03906187705947184the systematic error is -0.0030481040085144104 and the random error is 0.19672909246077736\n",
      "epoch = 29 train loss =0.04076335417451232 val loss =0.039059046319458446the systematic error is -0.0027083405580141874 and the random error is 0.1967268853267091\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "hidden_layers=(64,32)\n",
    "model=MLP(len(bvalues), hidden_layers, 1)\n",
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "momentum_coeff = 0.9\n",
    "change_weights = {layer: np.zeros_like(layer.weights) for layer in model.LM}\n",
    "change_bias = {layer: np.zeros_like(layer.bias) for layer in model.LM}\n",
    "\n",
    "# initialize wandb\n",
    "wandb.init(\n",
    "        project=\"AI_for_medical_imaging\", job_type=\"training\")\n",
    "\n",
    "loss_module1 = MSE()\n",
    "\n",
    "# set random seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# probe available devices\n",
    "if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# loop over epochs\n",
    "for epoch in range(epochs):\n",
    "    # initiate losses to 0\n",
    "    train_loss_f = 0\n",
    "    val_loss_f = 0\n",
    "    #loop over all training data\n",
    "    for x in trainloader:\n",
    "        # get data (x[0]) and put the data on the GPU if available\n",
    "        batch=x[0].numpy()\n",
    "        # get the reference f (x[2]) --> note x[1] and x[3] are D and Dp respectively\n",
    "        f_ref = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        # put the data through the neural network\n",
    "        f_pred = model.forward(batch)\n",
    "        # calculate loss (compare predicted f to the ground trueth)\n",
    "        train_loss_f += np.mean(loss_module1.forward(f_pred, f_ref))\n",
    "        # propogate the loss through the network (calculate d_wights/d_loss)\n",
    "        model.backward(loss_module1.backward())\n",
    "\n",
    "        #######################\n",
    "        # PUT YOUR CODE HERE  #\n",
    "        #######################\n",
    "        #updating of the gradients\n",
    "        for layer in model.LM: \n",
    "\n",
    "            new_change_weights = learning_rate * layer.grads['weight'] + momentum_coeff * change_weights[layer]\n",
    "            new_change_bias = learning_rate * layer.grads['bias'] + momentum_coeff * change_bias[layer]\n",
    "            \n",
    "            layer.weights -= new_change_weights\n",
    "            layer.bias -= new_change_bias\n",
    "\n",
    "            change_weights[layer] = new_change_weights\n",
    "            change_bias[layer] = new_change_bias\n",
    "\n",
    "        #######################\n",
    "        # END OF YOUR CODE    #\n",
    "        #######################\n",
    "        \n",
    "    # initialize error_metrics\n",
    "    SD_val=0\n",
    "    sys_val=0\n",
    "    for x in inferloader:\n",
    "        batch=x[0].numpy()\n",
    "        f_ref_val = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        f_pred_val = model.forward(batch)\n",
    "        val_loss_f += np.mean(loss_module1.forward(f_pred_val, f_ref_val))\n",
    "        SD, sys = hf.error_metrics(f_pred_val,f_ref_val)\n",
    "        SD_val += SD**2\n",
    "        sys_val += sys\n",
    "    SD_val = np.sqrt(SD_val/inferloader.__len__())\n",
    "    sys_val = sys_val/inferloader.__len__()\n",
    "\n",
    "    model.clear_cache()\n",
    "    \n",
    "    wandb.log({\"loss/train\": train_loss_f/trainloader.__len__(),\"loss/val\": val_loss_f/inferloader.__len__(),\"error/random error\":SD_val,\"error/systematic error\":sys_val})\n",
    "    print('epoch = ' + str(epoch) + ' train loss =' + str(train_loss_f/trainloader.__len__()) +' val loss =' + str(val_loss_f/inferloader.__len__()) + 'the systematic error is ' + str(sys_val) + ' and the random error is ' + str(SD_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE\n",
    "\n",
    "To show the effect of the added momentum, we run the code from C again, only now we are using a batch-size of 16. We are then comparing the network from C with the network from D. With a smaller batch-size the effect of the momentum is more evident, since the smaller batch-sizes create more noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e53da38e02e847d9bfe38203d2a93220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011167411111010652, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "CommError",
     "evalue": "Run initialization has timed out after 90.0 sec. Please try increasing the timeout with the `init_timeout` setting: `wandb.init(settings=wandb.Settings(init_timeout=120))`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCommError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#determine\u001b[39;00m\n\u001b[1;32m      9\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mAI_for_medical_imaging\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvisualize data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m run:\n\u001b[1;32m     12\u001b[0m     data_sim \u001b[38;5;241m=\u001b[39m hf\u001b[38;5;241m.\u001b[39msim_signal(SNR\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m40\u001b[39m),bvalues\u001b[38;5;241m=\u001b[39mbvalues,sims\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10000\u001b[39m,seed\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m10000\u001b[39m))\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# Only for visualisation purposes: here we create our \"Artifact\" in wandb --> this allows viewing the data in your wandb account\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/CLS/Y1/DL4MI/AI4MedicalImaging/.venv/lib/python3.13/site-packages/wandb/sdk/wandb_init.py:1485\u001b[0m, in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1481\u001b[0m     wl\u001b[38;5;241m.\u001b[39m_get_logger()\u001b[38;5;241m.\u001b[39mexception(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror in wandb.init()\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m   1483\u001b[0m \u001b[38;5;66;03m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[1;32m   1484\u001b[0m \u001b[38;5;66;03m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[0;32m-> 1485\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1486\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n",
      "File \u001b[0;32m~/Documents/CLS/Y1/DL4MI/AI4MedicalImaging/.venv/lib/python3.13/site-packages/wandb/analytics/sentry.py:156\u001b[0m, in \u001b[0;36mSentry.reraise\u001b[0;34m(self, exc)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexception(exc)\n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# but hopefully it's not too bad\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\u001b[38;5;241m.\u001b[39mwith_traceback(sys\u001b[38;5;241m.\u001b[39mexc_info()[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/Documents/CLS/Y1/DL4MI/AI4MedicalImaging/.venv/lib/python3.13/site-packages/wandb/sdk/wandb_init.py:1471\u001b[0m, in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[0m\n\u001b[1;32m   1468\u001b[0m         _monkeypatch_tensorboard()\n\u001b[1;32m   1469\u001b[0m         init_telemetry\u001b[38;5;241m.\u001b[39mfeature\u001b[38;5;241m.\u001b[39mtensorboard_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 1471\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_settings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1473\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1474\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m wl:\n",
      "File \u001b[0;32m~/Documents/CLS/Y1/DL4MI/AI4MedicalImaging/.venv/lib/python3.13/site-packages/wandb/sdk/wandb_init.py:960\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[0;34m(self, settings, config)\u001b[0m\n\u001b[1;32m    955\u001b[0m     run_init_handle\u001b[38;5;241m.\u001b[39m_cancel()\n\u001b[1;32m    957\u001b[0m     \u001b[38;5;66;03m# This may either be an issue with the W&B server (a CommError)\u001b[39;00m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;66;03m# or a bug in the SDK (an Error). We cannot distinguish between\u001b[39;00m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;66;03m# the two causes here.\u001b[39;00m\n\u001b[0;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CommError(\n\u001b[1;32m    961\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun initialization has timed out after \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m sec.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    962\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Please try increasing the timeout with the `init_timeout`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    963\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m setting: `wandb.init(settings=wandb.Settings(init_timeout=120))`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    964\u001b[0m     )\n\u001b[1;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error \u001b[38;5;241m:=\u001b[39m ProtobufErrorHandler\u001b[38;5;241m.\u001b[39mto_exception(result\u001b[38;5;241m.\u001b[39mrun_result\u001b[38;5;241m.\u001b[39merror):\n\u001b[1;32m    967\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[0;31mCommError\u001b[0m: Run initialization has timed out after 90.0 sec. Please try increasing the timeout with the `init_timeout` setting: `wandb.init(settings=wandb.Settings(init_timeout=120))`."
     ]
    }
   ],
   "source": [
    "# set b-values at which we \"measure\" (i.e. simulate signal)\n",
    "bvalues=[0, 10, 20, 30, 50, 75, 100, 150, 300, 500, 700, 1000]\n",
    "\n",
    "## Set the random seeds for reproducibility\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "#determine\n",
    "batch_size = 16\n",
    "\n",
    "with wandb.init(project=\"AI_for_medical_imaging\", job_type=\"visualize data\") as run:\n",
    "    data_sim = hf.sim_signal(SNR=(10,40),bvalues=bvalues,sims=10000,seed=np.random.randint(1,10000))\n",
    "    # Only for visualisation purposes: here we create our \"Artifact\" in wandb --> this allows viewing the data in your wandb account\n",
    "    for i in range(4):\n",
    "        #make b-value data pairs\n",
    "        example_data=[[x,y] for (x,y) in zip(bvalues,data_sim[0][i])]\n",
    "        # put it in a table\n",
    "        table = wandb.Table(data=example_data, columns=[\"b-values\", \"signal\"])\n",
    "        #tell wandb to plot the table\n",
    "        wandb.log({\"data_plot \" + str(i): wandb.plot.scatter(table, \"b-values\", \"signal\")})\n",
    "\n",
    "    # here we split the data into train (70%), test (15%) and validation (15%) sets\n",
    "    #split = int(np.floor(len(data_sim[0]) * 0.7))\n",
    "    train_set, test_set, val_set = torch.utils.data.random_split([[data_sim[0][i,:],data_sim[1][i],data_sim[2][i],data_sim[3][i]] for i in range(len(data_sim[3]))],[0.7,0.15,0.15])\n",
    "    #split = int(np.floor(len(rest) * 0.5))\n",
    "    #test_set, val_set = torch.utils.data.random_split([[rest[0][i,:],rest[1][i],rest[2][i],rest[3][i]] for i in range(len(rest[3]))],[split, len(rest[0]) - split])\n",
    "\n",
    "    # train loader loads the trianing data. We want to shuffle to make sure data order is modified each epoch and different data is selected each epoch.\n",
    "    trainloader = torch.utils.data.DataLoader(train_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   drop_last=True)\n",
    "    # validation data is loaded here. By not shuffling, we make sure the same data is loaded for validation every time. We can use substantially more data per batch as we are not training.\n",
    "    inferloader = torch.utils.data.DataLoader(val_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=False,\n",
    "                                   drop_last=True)\n",
    "        # validation data is loaded here. By not shuffling, we make sure the same data is loaded for validation every time. We can use substantially more data per batch as we are not training.\n",
    "    testloader = torch.utils.data.DataLoader(test_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=False,\n",
    "                                   drop_last=True)\n",
    "\n",
    "# define parameters\n",
    "hidden_layers=(64,32)\n",
    "model=MLP(len(bvalues), hidden_layers, 1)\n",
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "# initialize wandb\n",
    "wandb.init(\n",
    "        project=\"AI_for_medical_imaging\", job_type=\"training\")\n",
    "\n",
    "loss_module1 = MSE()\n",
    "\n",
    "# set random seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# probe available devices\n",
    "if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# loop over epochs\n",
    "for epoch in range(epochs):\n",
    "    # initiate losses to 0\n",
    "    train_loss_f = 0\n",
    "    val_loss_f = 0\n",
    "    #loop over all training data\n",
    "    for x in trainloader:\n",
    "        # get data (x[0]) and put the data on the GPU if available\n",
    "        batch=x[0].numpy()\n",
    "        # get the reference f (x[2]) --> note x[1] and x[3] are D and Dp respectively\n",
    "        f_ref = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        # put the data through the neural network\n",
    "        f_pred = model.forward(batch)\n",
    "        # calculate loss (compare predicted f to the ground trueth)\n",
    "        train_loss_f += np.mean(loss_module1.forward(f_pred, f_ref))\n",
    "        # propogate the loss through the network (calculate d_wights/d_loss)\n",
    "        model.backward(loss_module1.backward())\n",
    "\n",
    "        #######################\n",
    "        # PUT YOUR CODE HERE  #\n",
    "        #######################\n",
    "        #updating of the gradients\n",
    "        \n",
    "        for layer in model.LM: \n",
    "            layer.weights -= learning_rate * layer.grads['weight']\n",
    "            layer.bias -= learning_rate * layer.grads['bias']\n",
    "\n",
    "        #######################\n",
    "        # END OF YOUR CODE    #\n",
    "        #######################\n",
    "        \n",
    "    # initialize error_metrics\n",
    "    SD_val=0\n",
    "    sys_val=0\n",
    "    for x in inferloader:\n",
    "        batch=x[0].numpy()\n",
    "        f_ref_val = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        f_pred_val = model.forward(batch)\n",
    "        val_loss_f += np.mean(loss_module1.forward(f_pred_val, f_ref_val))\n",
    "        SD, sys = hf.error_metrics(f_pred_val,f_ref_val)\n",
    "        SD_val += SD**2\n",
    "        sys_val += sys\n",
    "    SD_val = np.sqrt(SD_val/inferloader.__len__())\n",
    "    sys_val = sys_val/inferloader.__len__()\n",
    "\n",
    "    model.clear_cache()\n",
    "    \n",
    "    wandb.log({\"loss/train\": train_loss_f/trainloader.__len__(),\"loss/val\": val_loss_f/inferloader.__len__(),\"error/random error\":SD_val,\"error/systematic error\":sys_val})\n",
    "    print('epoch = ' + str(epoch) + ' train loss =' + str(train_loss_f/trainloader.__len__()) +' val loss =' + str(val_loss_f/inferloader.__len__()) + 'the systematic error is ' + str(sys_val) + ' and the random error is ' + str(SD_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define parameters\n",
    "hidden_layers=(64,32)\n",
    "model=MLP(len(bvalues), hidden_layers, 1)\n",
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "momentum_coeff = 0.9\n",
    "change_weights = {layer: np.zeros_like(layer.weights) for layer in model.LM}\n",
    "change_bias = {layer: np.zeros_like(layer.bias) for layer in model.LM}\n",
    "\n",
    "# initialize wandb\n",
    "wandb.init(\n",
    "        project=\"AI_for_medical_imaging\", job_type=\"training\")\n",
    "\n",
    "loss_module1 = MSE()\n",
    "\n",
    "# set random seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# probe available devices\n",
    "if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# loop over epochs\n",
    "for epoch in range(epochs):\n",
    "    # initiate losses to 0\n",
    "    train_loss_f = 0\n",
    "    val_loss_f = 0\n",
    "    #loop over all training data\n",
    "    for x in trainloader:\n",
    "        # get data (x[0]) and put the data on the GPU if available\n",
    "        batch=x[0].numpy()\n",
    "        # get the reference f (x[2]) --> note x[1] and x[3] are D and Dp respectively\n",
    "        f_ref = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        # put the data through the neural network\n",
    "        f_pred = model.forward(batch)\n",
    "        # calculate loss (compare predicted f to the ground trueth)\n",
    "        train_loss_f += np.mean(loss_module1.forward(f_pred, f_ref))\n",
    "        # propogate the loss through the network (calculate d_wights/d_loss)\n",
    "        model.backward(loss_module1.backward())\n",
    "\n",
    "        #######################\n",
    "        # PUT YOUR CODE HERE  #\n",
    "        #######################\n",
    "        #updating of the gradients\n",
    "        for layer in model.LM: \n",
    "\n",
    "            new_change_weights = learning_rate * layer.grads['weight'] + momentum_coeff * change_weights[layer]\n",
    "            new_change_bias = learning_rate * layer.grads['bias'] + momentum_coeff * change_bias[layer]\n",
    "            \n",
    "            layer.weights -= new_change_weights\n",
    "            layer.bias -= new_change_bias\n",
    "\n",
    "            change_weights[layer] = new_change_weights\n",
    "            change_bias[layer] = new_change_bias\n",
    "\n",
    "        #######################\n",
    "        # END OF YOUR CODE    #\n",
    "        #######################\n",
    "        \n",
    "    # initialize error_metrics\n",
    "    SD_val=0\n",
    "    sys_val=0\n",
    "    for x in inferloader:\n",
    "        batch=x[0].numpy()\n",
    "        f_ref_val = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        f_pred_val = model.forward(batch)\n",
    "        val_loss_f += np.mean(loss_module1.forward(f_pred_val, f_ref_val))\n",
    "        SD, sys = hf.error_metrics(f_pred_val,f_ref_val)\n",
    "        SD_val += SD**2\n",
    "        sys_val += sys\n",
    "    SD_val = np.sqrt(SD_val/inferloader.__len__())\n",
    "    sys_val = sys_val/inferloader.__len__()\n",
    "\n",
    "    model.clear_cache()\n",
    "    \n",
    "    wandb.log({\"loss/train\": train_loss_f/trainloader.__len__(),\"loss/val\": val_loss_f/inferloader.__len__(),\"error/random error\":SD_val,\"error/systematic error\":sys_val})\n",
    "    print('epoch = ' + str(epoch) + ' train loss =' + str(train_loss_f/trainloader.__len__()) +' val loss =' + str(val_loss_f/inferloader.__len__()) + 'the systematic error is ' + str(sys_val) + ' and the random error is ' + str(SD_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
