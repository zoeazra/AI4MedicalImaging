{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# MIT License\n",
    "#\n",
    "# Copyright (c) 2021 University of Amsterdam\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to conditions.\n",
    "#\n",
    "# Author: Deep Learning Course (UvA) | Fall 2022 & Oliver Gurney-Champion | Spring 2023\n",
    "# Date modified: Jan 2023\n",
    "################################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Import requiered packages\n",
    "imports the packages and sets the random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import random\n",
    "import numpy as np\n",
    "import helper_functions as hf\n",
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# set random seed\n",
    "seed =42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "login to your free wandb account. Note you will need to set up your account on https://wandb.ai/authorize\n",
    "wandb allows you to keep track of your neural network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Simulate and view the IVIM data\n",
    "Here, we split our data into a training set, validation set and test set. Note that the current implementation only uses the training set and it is up to you (in your exercises) to also implement the validation and test run. At this point, we already split the data for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>error/random error</td><td>███████████████▃▁▁█████▇▆▁▁█████████████</td></tr><tr><td>error/systematic error</td><td>▁▅▇▇██████████▇██▇▇▇████▇▇█▇▇▇██████▁▅▆▇</td></tr><tr><td>loss/train</td><td>█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▁▃▃▃▃▃▃▃▃▁▁█▄▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>loss/val</td><td>█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▄▃▁▁▅▅▅▅▅▅▅▅▃▅▅▅▅▅▅▅▅▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>error/random error</td><td>0.19508</td></tr><tr><td>error/systematic error</td><td>-0.00953</td></tr><tr><td>loss/train</td><td>0.04119</td></tr><tr><td>loss/val</td><td>0.04018</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">wild-tree-194</strong> at: <a href='https://wandb.ai/zoeazra-vu/AI_for_medical_imaging/runs/y2m6orzc' target=\"_blank\">https://wandb.ai/zoeazra-vu/AI_for_medical_imaging/runs/y2m6orzc</a><br> View project at: <a href='https://wandb.ai/zoeazra-vu/AI_for_medical_imaging' target=\"_blank\">https://wandb.ai/zoeazra-vu/AI_for_medical_imaging</a><br>Synced 5 W&B file(s), 4 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250218_162151-y2m6orzc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set b-values at which we \"measure\" (i.e. simulate signal)\n",
    "bvalues=[0, 10, 20, 30, 50, 75, 100, 150, 300, 500, 700, 1000]\n",
    "\n",
    "## Set the random seeds for reproducibility\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "#determine\n",
    "batch_size = 128\n",
    "\n",
    "with wandb.init(project=\"AI_for_medical_imaging\", job_type=\"visualize data\") as run:\n",
    "    data_sim = hf.sim_signal(SNR=(10,40),bvalues=bvalues,sims=10000,seed=np.random.randint(1,10000))\n",
    "    # Only for visualisation purposes: here we create our \"Artifact\" in wandb --> this allows viewing the data in your wandb account\n",
    "    for i in range(4):\n",
    "        #make b-value data pairs\n",
    "        example_data=[[x,y] for (x,y) in zip(bvalues,data_sim[0][i])]\n",
    "        # put it in a table\n",
    "        table = wandb.Table(data=example_data, columns=[\"b-values\", \"signal\"])\n",
    "        #tell wandb to plot the table\n",
    "        wandb.log({\"data_plot \" + str(i): wandb.plot.scatter(table, \"b-values\", \"signal\")})\n",
    "\n",
    "    # here we split the data into train (70%), test (15%) and validation (15%) sets\n",
    "    #split = int(np.floor(len(data_sim[0]) * 0.7))\n",
    "    train_set, test_set, val_set = torch.utils.data.random_split([[data_sim[0][i,:],data_sim[1][i],data_sim[2][i],data_sim[3][i]] for i in range(len(data_sim[3]))],[0.7,0.15,0.15])\n",
    "    #split = int(np.floor(len(rest) * 0.5))\n",
    "    #test_set, val_set = torch.utils.data.random_split([[rest[0][i,:],rest[1][i],rest[2][i],rest[3][i]] for i in range(len(rest[3]))],[split, len(rest[0]) - split])\n",
    "\n",
    "    # train loader loads the trianing data. We want to shuffle to make sure data order is modified each epoch and different data is selected each epoch.\n",
    "    trainloader = torch.utils.data.DataLoader(train_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   drop_last=True)\n",
    "    # validation data is loaded here. By not shuffling, we make sure the same data is loaded for validation every time. We can use substantially more data per batch as we are not training.\n",
    "    inferloader = torch.utils.data.DataLoader(val_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=False,\n",
    "                                   drop_last=True)\n",
    "        # validation data is loaded here. By not shuffling, we make sure the same data is loaded for validation every time. We can use substantially more data per batch as we are not training.\n",
    "    testloader = torch.utils.data.DataLoader(test_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=False,\n",
    "                                   drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "3 A.\tProgram the different neural network modules (Lineairemodule, Relumodule, Tanhmodule, MSE) in “modules.py”. Use “do_unittest.py” to test whether the forward and backward passes are correct. Make only use of matrix multiplications, no for loops. Points will be deduced when for-loops are used where matrix multiplications were possible.\n",
    "\n",
    "3 B.  Combine all modules into a multilayer perceptron (MLP) in “exercise_3.ipynb”. Use ReLU activation functions after each fully connected linear module, except for the last module, after which a Tanh module should be used (constraining parameters between [-1, 1]). The number of layers and number of nodes per layer should be adjustable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This module implements a multi-layer perceptron (MLP) in NumPy.\n",
    "You should fill in code into indicated sections.\n",
    "\"\"\"\n",
    "from modules import MLP #you need to adapt these\n",
    "from modules import MSE \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Exercise 3C-D\n",
    "C.\tTrain the neural network using just python and numpy code. You can find modules in \"modules.py\" which you will first have to fill in and train. You can then use these modules here!\n",
    "\n",
    "You can use \"do_unittest.py\" to test whether the modules from \"modules.py\" have been implemented correctly.\n",
    "\n",
    "You will need to add the updating of the gradients. As a sanity check, a network with 2 layers (64, 32), combined with a lr of 0.001, 30 epochs, and a batch size of 128 should work reasonably well and result in a systematic error of around 0.2% and a random error of around 20%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 train loss =0.15110451695738725 val loss =0.13967357859094745the systematic error is -0.31718562974888537 and the random error is 0.19676503107902998\n",
      "epoch = 1 train loss =0.12933362420408548 val loss =0.12025642650770227the systematic error is -0.2849268648928854 and the random error is 0.19678077472318645\n",
      "epoch = 2 train loss =0.1121316604476833 val loss =0.10468654363875869the systematic error is -0.2561475779295251 and the random error is 0.19678545903920672\n",
      "epoch = 3 train loss =0.0983904405573263 val loss =0.09221528707486172the systematic error is -0.2305176897293819 and the random error is 0.19679031049906748\n",
      "epoch = 4 train loss =0.08737897447624292 val loss =0.08222906062450137the systematic error is -0.20772855253327566 and the random error is 0.19679290461767548\n",
      "epoch = 5 train loss =0.07861356029572032 val loss =0.07422724175725935the systematic error is -0.18748024761041301 and the random error is 0.19679413857179398\n",
      "epoch = 6 train loss =0.07170710960158146 val loss =0.06778347331528328the systematic error is -0.16942532017882647 and the random error is 0.19679447849489262\n",
      "epoch = 7 train loss =0.06581387427468977 val loss =0.06260646166661951the systematic error is -0.15338867699886594 and the random error is 0.19679407397006882\n",
      "epoch = 8 train loss =0.06138803825595004 val loss =0.05840881358523697the systematic error is -0.13903542259539442 and the random error is 0.1967930323210873\n",
      "epoch = 9 train loss =0.057586462446808144 val loss =0.05501130837372421the systematic error is -0.12622984815845195 and the random error is 0.19679143274771982\n",
      "epoch = 10 train loss =0.054605440940893306 val loss =0.05224432273597805the systematic error is -0.11475095567640702 and the random error is 0.1967894266565915\n",
      "epoch = 11 train loss =0.05217562355425039 val loss =0.04998700083187842the systematic error is -0.10445774260560257 and the random error is 0.19678700260598078\n",
      "epoch = 12 train loss =0.050111273493543926 val loss =0.04814338858658076the systematic error is -0.09523056058256073 and the random error is 0.19678435253621387\n",
      "epoch = 13 train loss =0.04855058789167827 val loss =0.046625547825631the systematic error is -0.08690293333606541 and the random error is 0.19678157931761864\n",
      "epoch = 14 train loss =0.04727669215338078 val loss =0.045369942060458135the systematic error is -0.07935735493534991 and the random error is 0.19677887008136127\n",
      "epoch = 15 train loss =0.04619149190281108 val loss =0.04433352260452543the systematic error is -0.0725409115993644 and the random error is 0.1967763152408193\n",
      "epoch = 16 train loss =0.045275970129757204 val loss =0.04348283122008923the systematic error is -0.06642571058663506 and the random error is 0.1967740895163471\n",
      "epoch = 17 train loss =0.04443145649006031 val loss =0.04277576678193154the systematic error is -0.060877279371788724 and the random error is 0.19677224231418103\n",
      "epoch = 18 train loss =0.0439533644804646 val loss =0.042187833398959235the systematic error is -0.05584565711724483 and the random error is 0.196770658403443\n",
      "epoch = 19 train loss =0.04339322646882296 val loss =0.04169638485886352the systematic error is -0.05126201980012917 and the random error is 0.19676938013661502\n",
      "epoch = 20 train loss =0.042981960199691104 val loss =0.04128382846614959the systematic error is -0.04707091550771954 and the random error is 0.1967682875182945\n",
      "epoch = 21 train loss =0.04262154801536643 val loss =0.040944725911076105the systematic error is -0.043324103603039635 and the random error is 0.19676725356936917\n",
      "epoch = 22 train loss =0.04231177290159889 val loss =0.040656080166604186the systematic error is -0.039857913846505104 and the random error is 0.19676644258171963\n",
      "epoch = 23 train loss =0.042089539098154646 val loss =0.04041701717520384the systematic error is -0.0367410923746681 and the random error is 0.1967656402586933\n",
      "epoch = 24 train loss =0.04197917265766857 val loss =0.04021672149918776the systematic error is -0.0339100833887666 and the random error is 0.19676492960846403\n",
      "epoch = 25 train loss =0.04174998659152815 val loss =0.04004608031191605the systematic error is -0.031296956050699425 and the random error is 0.19676430669060094\n",
      "epoch = 26 train loss =0.04152693430600567 val loss =0.03990443201643992the systematic error is -0.028949794651946133 and the random error is 0.19676370236525131\n",
      "epoch = 27 train loss =0.04145370346041313 val loss =0.039781945107942515the systematic error is -0.026754766925938 and the random error is 0.1967631621355263\n",
      "epoch = 28 train loss =0.04133593247398752 val loss =0.03967971498394247the systematic error is -0.024774826560816223 and the random error is 0.19676264523311174\n",
      "epoch = 29 train loss =0.04121224413557597 val loss =0.039592056671448675the systematic error is -0.022941813226515763 and the random error is 0.1967621581594563\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "hidden_layers=(64,32)\n",
    "model=MLP(len(bvalues), hidden_layers, 1)\n",
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "# initialize wandb\n",
    "wandb.init(\n",
    "        project=\"AI_for_medical_imaging\", job_type=\"training\")\n",
    "\n",
    "loss_module1 = MSE()\n",
    "\n",
    "# set random seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# probe available devices\n",
    "if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# loop over epochs\n",
    "for epoch in range(epochs):\n",
    "    # initiate losses to 0\n",
    "    train_loss_f = 0\n",
    "    val_loss_f = 0\n",
    "    #loop over all training data\n",
    "    for x in trainloader:\n",
    "        # get data (x[0]) and put the data on the GPU if available\n",
    "        batch=x[0].numpy()\n",
    "        # get the reference f (x[2]) --> note x[1] and x[3] are D and Dp respectively\n",
    "        f_ref = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        # put the data through the neural network\n",
    "        f_pred = model.forward(batch)\n",
    "        # calculate loss (compare predicted f to the ground trueth)\n",
    "        train_loss_f += np.mean(loss_module1.forward(f_pred, f_ref))\n",
    "        # propogate the loss through the network (calculate d_wights/d_loss)\n",
    "        model.backward(loss_module1.backward())\n",
    "\n",
    "        #######################\n",
    "        # PUT YOUR CODE HERE  #\n",
    "        #######################\n",
    "        #updating of the gradients\n",
    "        \n",
    "        for layer in model.LM: \n",
    "            layer.weights -= learning_rate * layer.grads['weight']\n",
    "            layer.bias -= learning_rate * layer.grads['bias']\n",
    "\n",
    "        #######################\n",
    "        # END OF YOUR CODE    #\n",
    "        #######################\n",
    "        \n",
    "    # initialize error_metrics\n",
    "    SD_val=0\n",
    "    sys_val=0\n",
    "    for x in inferloader:\n",
    "        batch=x[0].numpy()\n",
    "        f_ref_val = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        f_pred_val = model.forward(batch)\n",
    "        val_loss_f += np.mean(loss_module1.forward(f_pred_val, f_ref_val))\n",
    "        SD, sys = hf.error_metrics(f_pred_val,f_ref_val)\n",
    "        SD_val += SD**2\n",
    "        sys_val += sys\n",
    "    SD_val = np.sqrt(SD_val/inferloader.__len__())\n",
    "    sys_val = sys_val/inferloader.__len__()\n",
    "\n",
    "    model.clear_cache()\n",
    "    \n",
    "    wandb.log({\"loss/train\": train_loss_f/trainloader.__len__(),\"loss/val\": val_loss_f/inferloader.__len__(),\"error/random error\":SD_val,\"error/systematic error\":sys_val})\n",
    "    print('epoch = ' + str(epoch) + ' train loss =' + str(train_loss_f/trainloader.__len__()) +' val loss =' + str(val_loss_f/inferloader.__len__()) + 'the systematic error is ' + str(sys_val) + ' and the random error is ' + str(SD_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D.\tIt now uses the mini-batch stochastic gradient descent algorithm. Add momentum to the training update of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 train loss =0.10312214855702172 val loss =0.05560625150320084the systematic error is -0.12854279880753952 and the random error is 0.19680577501333985\n",
      "epoch = 1 train loss =0.046750459772518065 val loss =0.04078000140690448the systematic error is -0.04121112762335953 and the random error is 0.19680234953663206\n",
      "epoch = 2 train loss =0.041431747821242135 val loss =0.039372700060214934the systematic error is -0.017150636223985036 and the random error is 0.19679457852070997\n",
      "epoch = 3 train loss =0.04083683882174035 val loss =0.03914080348080447the systematic error is -0.00793795470707066 and the random error is 0.19679265411365188\n",
      "epoch = 4 train loss =0.040745308635939205 val loss =0.03910012618253933the systematic error is -0.00482494276550008 and the random error is 0.19679027625426668\n",
      "epoch = 5 train loss =0.04085400873604819 val loss =0.03908567556208269the systematic error is -0.003515116936150102 and the random error is 0.1967813888917374\n",
      "epoch = 6 train loss =0.040911618807435476 val loss =0.03908482697006589the systematic error is -0.003540723444390855 and the random error is 0.19677879899631018\n",
      "epoch = 7 train loss =0.040876023738989305 val loss =0.03908240885342942the systematic error is -0.003315067531773042 and the random error is 0.19677660170242692\n",
      "epoch = 8 train loss =0.04084998481560338 val loss =0.03907879267440275the systematic error is -0.002870134818148949 and the random error is 0.19677442155924513\n",
      "epoch = 9 train loss =0.040848210662789576 val loss =0.03907911998519817the systematic error is -0.0030735594263743863 and the random error is 0.19677219691501793\n",
      "epoch = 10 train loss =0.04087466392185478 val loss =0.03908233628474167the systematic error is -0.0036835239751094716 and the random error is 0.19676991293311302\n",
      "epoch = 11 train loss =0.040857811850630636 val loss =0.03907671850378174the systematic error is -0.0029685787026015927 and the random error is 0.19676773883421353\n",
      "epoch = 12 train loss =0.04084920447558115 val loss =0.03907998208306979the systematic error is -0.003601090397076449 and the random error is 0.19676548875391311\n",
      "epoch = 13 train loss =0.040852808327471436 val loss =0.039077071918940394the systematic error is -0.003302363167707372 and the random error is 0.19676334916347496\n",
      "epoch = 14 train loss =0.040827950226267624 val loss =0.039072128401079625the systematic error is -0.0026072965854963682 and the random error is 0.19676124056071267\n",
      "epoch = 15 train loss =0.040836986438843474 val loss =0.03907209711932981the systematic error is -0.002756445839245093 and the random error is 0.19675914299769479\n",
      "epoch = 16 train loss =0.040907362961056445 val loss =0.03907760424287377the systematic error is -0.0037358161460244806 and the random error is 0.1967569956940372\n",
      "epoch = 17 train loss =0.04077211069220376 val loss =0.039074914035717785the systematic error is -0.0034733340367969086 and the random error is 0.19675498261314173\n",
      "epoch = 18 train loss =0.040902640320090555 val loss =0.03907128147792174the systematic error is -0.0030378710426206216 and the random error is 0.19675297132612987\n",
      "epoch = 19 train loss =0.04084201232529653 val loss =0.039074792489534396the systematic error is -0.0036819995116716725 and the random error is 0.19675090892890837\n",
      "epoch = 20 train loss =0.040815800953671696 val loss =0.03906800011136733the systematic error is -0.002745238204765826 and the random error is 0.19674896188596766\n",
      "epoch = 21 train loss =0.04086064557510057 val loss =0.0390681950086827the systematic error is -0.0029203411655625896 and the random error is 0.19674695071399045\n",
      "epoch = 22 train loss =0.04080044237666188 val loss =0.03907189393656588the systematic error is -0.00361205184682689 and the random error is 0.1967448829260279\n",
      "epoch = 23 train loss =0.040845387257716845 val loss =0.03906538043402224the systematic error is -0.0027651847437936273 and the random error is 0.19674207474818972\n",
      "epoch = 24 train loss =0.04094398059283885 val loss =0.039070767486013284the systematic error is -0.0038310310023792343 and the random error is 0.1967379336407114\n",
      "epoch = 25 train loss =0.040860599387610094 val loss =0.03906536883408452the systematic error is -0.0031831750814280742 and the random error is 0.19673577802983921\n",
      "epoch = 26 train loss =0.0408012524937932 val loss =0.03906806936179899the systematic error is -0.003706509331181318 and the random error is 0.196733494767394\n",
      "epoch = 27 train loss =0.04081468278494488 val loss =0.039062192648417524the systematic error is -0.00295212469236472 and the random error is 0.19673134148964921\n",
      "epoch = 28 train loss =0.040811766454961665 val loss =0.03906187705947184the systematic error is -0.0030481040085144104 and the random error is 0.19672909246077736\n",
      "epoch = 29 train loss =0.04076335417451232 val loss =0.039059046319458446the systematic error is -0.0027083405580141874 and the random error is 0.1967268853267091\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "hidden_layers=(64,32)\n",
    "model=MLP(len(bvalues), hidden_layers, 1)\n",
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "momentum_coeff = 0.9\n",
    "change_weights = {layer: np.zeros_like(layer.weights) for layer in model.LM}\n",
    "change_bias = {layer: np.zeros_like(layer.bias) for layer in model.LM}\n",
    "\n",
    "# initialize wandb\n",
    "wandb.init(\n",
    "        project=\"AI_for_medical_imaging\", job_type=\"training\")\n",
    "\n",
    "loss_module1 = MSE()\n",
    "\n",
    "# set random seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# probe available devices\n",
    "if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# loop over epochs\n",
    "for epoch in range(epochs):\n",
    "    # initiate losses to 0\n",
    "    train_loss_f = 0\n",
    "    val_loss_f = 0\n",
    "    #loop over all training data\n",
    "    for x in trainloader:\n",
    "        # get data (x[0]) and put the data on the GPU if available\n",
    "        batch=x[0].numpy()\n",
    "        # get the reference f (x[2]) --> note x[1] and x[3] are D and Dp respectively\n",
    "        f_ref = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        # put the data through the neural network\n",
    "        f_pred = model.forward(batch)\n",
    "        # calculate loss (compare predicted f to the ground trueth)\n",
    "        train_loss_f += np.mean(loss_module1.forward(f_pred, f_ref))\n",
    "        # propogate the loss through the network (calculate d_wights/d_loss)\n",
    "        model.backward(loss_module1.backward())\n",
    "\n",
    "        #######################\n",
    "        # PUT YOUR CODE HERE  #\n",
    "        #######################\n",
    "        #updating of the gradients\n",
    "        for layer in model.LM: \n",
    "\n",
    "            new_change_weights = learning_rate * layer.grads['weight'] + momentum_coeff * change_weights[layer]\n",
    "            new_change_bias = learning_rate * layer.grads['bias'] + momentum_coeff * change_bias[layer]\n",
    "            \n",
    "            layer.weights -= new_change_weights\n",
    "            layer.bias -= new_change_bias\n",
    "\n",
    "            change_weights[layer] = new_change_weights\n",
    "            change_bias[layer] = new_change_bias\n",
    "\n",
    "        #######################\n",
    "        # END OF YOUR CODE    #\n",
    "        #######################\n",
    "        \n",
    "    # initialize error_metrics\n",
    "    SD_val=0\n",
    "    sys_val=0\n",
    "    for x in inferloader:\n",
    "        batch=x[0].numpy()\n",
    "        f_ref_val = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        f_pred_val = model.forward(batch)\n",
    "        val_loss_f += np.mean(loss_module1.forward(f_pred_val, f_ref_val))\n",
    "        SD, sys = hf.error_metrics(f_pred_val,f_ref_val)\n",
    "        SD_val += SD**2\n",
    "        sys_val += sys\n",
    "    SD_val = np.sqrt(SD_val/inferloader.__len__())\n",
    "    sys_val = sys_val/inferloader.__len__()\n",
    "\n",
    "    model.clear_cache()\n",
    "    \n",
    "    wandb.log({\"loss/train\": train_loss_f/trainloader.__len__(),\"loss/val\": val_loss_f/inferloader.__len__(),\"error/random error\":SD_val,\"error/systematic error\":sys_val})\n",
    "    print('epoch = ' + str(epoch) + ' train loss =' + str(train_loss_f/trainloader.__len__()) +' val loss =' + str(val_loss_f/inferloader.__len__()) + 'the systematic error is ' + str(sys_val) + ' and the random error is ' + str(SD_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE\n",
    "\n",
    "To show the effect of the added momentum, we run the code from C again, only now we are using a batch-size of 16. We are then comparing the network from C with the network from D. With a smaller batch-size the effect of the momentum is more evident, since the smaller batch-sizes create more noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>error/random error</td><td>████████████████████▇▇▆▅▃▁▁▁█████████▆▄▁</td></tr><tr><td>error/systematic error</td><td>▁▄▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▆▆▆▇▇▇▇▇▇█▇██▇▆▇▆▆</td></tr><tr><td>loss/train</td><td>█▄▄▃▃▃▃▃▃▃▃▃▃▃▄▃▃▃▃▃▃▃▃▃▃▁▃▃▃▃▃▃▃▃▃▃▃▂▂▁</td></tr><tr><td>loss/val</td><td>█▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▃▂▁▁▅▅▅▅▅▅▅▅▄▃▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>error/random error</td><td>0.0981</td></tr><tr><td>error/systematic error</td><td>-0.00484</td></tr><tr><td>loss/train</td><td>0.01056</td></tr><tr><td>loss/val</td><td>0.0103</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">polished-dawn-573</strong> at: <a href='https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging/runs/2n6hr93u' target=\"_blank\">https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging/runs/2n6hr93u</a><br> View project at: <a href='https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging' target=\"_blank\">https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging</a><br>Synced 5 W&B file(s), 4 media file(s), 8 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250218_162748-2n6hr93u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf50954aa6cc4ddc8b2c454d22923841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011156750455524566, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/elenaliarou/Documents/master/block4/dl/AI4MedicalImaging/Assignment 1/wandb/run-20250218_162844-khqxl5db</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging/runs/khqxl5db' target=\"_blank\">avid-snow-574</a></strong> to <a href='https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging' target=\"_blank\">https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging/runs/khqxl5db' target=\"_blank\">https://wandb.ai/elenaliarou26-vrije-universiteit-amsterdam/AI_for_medical_imaging/runs/khqxl5db</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 train loss =0.09898557806678354 val loss =0.06241541259889638the systematic error is -0.1493571862058027 and the random error is 0.1951254733695247\n",
      "epoch = 1 train loss =0.05206255856947782 val loss =0.04484842332759573the systematic error is -0.06885281683604144 and the random error is 0.19512516733504306\n",
      "epoch = 2 train loss =0.04344828695544876 val loss =0.04123015266757754the systematic error is -0.03353883426699769 and the random error is 0.1951192484291124\n",
      "epoch = 3 train loss =0.041597296040805205 val loss =0.040384686978869606the systematic error is -0.016796475396452955 and the random error is 0.19511263655667765\n",
      "epoch = 4 train loss =0.04118826418239022 val loss =0.040183969804450374the systematic error is -0.009226126518475855 and the random error is 0.19510353922728166\n",
      "epoch = 5 train loss =0.04107053000543822 val loss =0.04012336882422833the systematic error is -0.005231887604367496 and the random error is 0.19509652088939053\n",
      "epoch = 6 train loss =0.0410224139607907 val loss =0.04010638998850011the systematic error is -0.0036172435981193956 and the random error is 0.19508991267854717\n",
      "epoch = 7 train loss =0.0410176828130296 val loss =0.0400968123711641the systematic error is -0.0025144721300019695 and the random error is 0.19508299052581826\n",
      "epoch = 8 train loss =0.04100105453154009 val loss =0.040090665402806674the systematic error is -0.0016717105651067692 and the random error is 0.1950765621406459\n",
      "epoch = 9 train loss =0.04101771282632658 val loss =0.040086352323990854the systematic error is -0.001002321747606702 and the random error is 0.19507036933718516\n",
      "epoch = 10 train loss =0.04104594363622611 val loss =0.04008362967796665the systematic error is -0.0009165571865544707 and the random error is 0.19506409017048276\n",
      "epoch = 11 train loss =0.04100672698581457 val loss =0.04008056120473291the systematic error is -0.0005248418510590249 and the random error is 0.19505794465589452\n",
      "epoch = 12 train loss =0.041023634488611946 val loss =0.040077909995576144the systematic error is -0.00038029792107148745 and the random error is 0.19505175787790335\n",
      "epoch = 13 train loss =0.04101252058660838 val loss =0.04007648627272046the systematic error is -0.001149613773811959 and the random error is 0.19504537331660102\n",
      "epoch = 14 train loss =0.040985075368195005 val loss =0.04007375860979529the systematic error is -0.0010500728959090988 and the random error is 0.19503921491819673\n",
      "epoch = 15 train loss =0.04102578152968561 val loss =0.04007177487851472the systematic error is -0.00128779245975986 and the random error is 0.1950329821350217\n",
      "epoch = 16 train loss =0.04101085486331637 val loss =0.040068579782630065the systematic error is -0.0009715926094530967 and the random error is 0.19502689436911877\n",
      "epoch = 17 train loss =0.04100169612872204 val loss =0.04006699501654643the systematic error is -0.0013787794769445664 and the random error is 0.1950206585477493\n",
      "epoch = 18 train loss =0.04100666582290515 val loss =0.04006549037274407the systematic error is -0.001711188490913602 and the random error is 0.1950144454391778\n",
      "epoch = 19 train loss =0.041000222189742086 val loss =0.040061909688389484the systematic error is -0.0013449219885372885 and the random error is 0.19500840407975512\n",
      "epoch = 20 train loss =0.04099909703547616 val loss =0.04005923853111461the systematic error is -0.0012745710799375804 and the random error is 0.19500230071261496\n",
      "epoch = 21 train loss =0.04097871524604259 val loss =0.04005577508708937the systematic error is -0.0007810527846055035 and the random error is 0.1949962916118416\n",
      "epoch = 22 train loss =0.04101067353163728 val loss =0.04005361765776506the systematic error is -0.0009921325984474651 and the random error is 0.19499007938601334\n",
      "epoch = 23 train loss =0.04097621423573276 val loss =0.04005111128324979the systematic error is -0.000997986462786241 and the random error is 0.19498390125105275\n",
      "epoch = 24 train loss =0.04100181538654741 val loss =0.040048398398844234the systematic error is -0.000895289803618107 and the random error is 0.19497772142625888\n",
      "epoch = 25 train loss =0.040982607441658686 val loss =0.04004649441746858the systematic error is -0.0012140011621966916 and the random error is 0.19497139801910282\n",
      "epoch = 26 train loss =0.040976945400369016 val loss =0.04004442040063376the systematic error is -0.0014081206410245025 and the random error is 0.19496505754245458\n",
      "epoch = 27 train loss =0.0409822442642907 val loss =0.04004043718013088the systematic error is -0.0007114295516344248 and the random error is 0.19495890470273086\n",
      "epoch = 28 train loss =0.040992830469580195 val loss =0.04003770894115996the systematic error is -0.0006154714396318383 and the random error is 0.19495251923596232\n",
      "epoch = 29 train loss =0.04095660828016239 val loss =0.04003541863544592the systematic error is -0.000867778735648556 and the random error is 0.19494597779817016\n"
     ]
    }
   ],
   "source": [
    "# set b-values at which we \"measure\" (i.e. simulate signal)\n",
    "bvalues=[0, 10, 20, 30, 50, 75, 100, 150, 300, 500, 700, 1000]\n",
    "\n",
    "## Set the random seeds for reproducibility\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "#determine\n",
    "batch_size = 16\n",
    "\n",
    "with wandb.init(project=\"AI_for_medical_imaging\", job_type=\"visualize data\") as run:\n",
    "    data_sim = hf.sim_signal(SNR=(10,40),bvalues=bvalues,sims=10000,seed=np.random.randint(1,10000))\n",
    "    # Only for visualisation purposes: here we create our \"Artifact\" in wandb --> this allows viewing the data in your wandb account\n",
    "    for i in range(4):\n",
    "        #make b-value data pairs\n",
    "        example_data=[[x,y] for (x,y) in zip(bvalues,data_sim[0][i])]\n",
    "        # put it in a table\n",
    "        table = wandb.Table(data=example_data, columns=[\"b-values\", \"signal\"])\n",
    "        #tell wandb to plot the table\n",
    "        wandb.log({\"data_plot \" + str(i): wandb.plot.scatter(table, \"b-values\", \"signal\")})\n",
    "\n",
    "    # here we split the data into train (70%), test (15%) and validation (15%) sets\n",
    "    #split = int(np.floor(len(data_sim[0]) * 0.7))\n",
    "    train_set, test_set, val_set = torch.utils.data.random_split([[data_sim[0][i,:],data_sim[1][i],data_sim[2][i],data_sim[3][i]] for i in range(len(data_sim[3]))],[0.7,0.15,0.15])\n",
    "    #split = int(np.floor(len(rest) * 0.5))\n",
    "    #test_set, val_set = torch.utils.data.random_split([[rest[0][i,:],rest[1][i],rest[2][i],rest[3][i]] for i in range(len(rest[3]))],[split, len(rest[0]) - split])\n",
    "\n",
    "    # train loader loads the trianing data. We want to shuffle to make sure data order is modified each epoch and different data is selected each epoch.\n",
    "    trainloader = torch.utils.data.DataLoader(train_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   drop_last=True)\n",
    "    # validation data is loaded here. By not shuffling, we make sure the same data is loaded for validation every time. We can use substantially more data per batch as we are not training.\n",
    "    inferloader = torch.utils.data.DataLoader(val_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=False,\n",
    "                                   drop_last=True)\n",
    "        # validation data is loaded here. By not shuffling, we make sure the same data is loaded for validation every time. We can use substantially more data per batch as we are not training.\n",
    "    testloader = torch.utils.data.DataLoader(test_set,\n",
    "                                   batch_size=batch_size,\n",
    "                                   shuffle=False,\n",
    "                                   drop_last=True)\n",
    "\n",
    "# define parameters\n",
    "hidden_layers=(64,32)\n",
    "model=MLP(len(bvalues), hidden_layers, 1)\n",
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "\n",
    "# initialize wandb\n",
    "wandb.init(\n",
    "        project=\"AI_for_medical_imaging\", job_type=\"training\")\n",
    "\n",
    "loss_module1 = MSE()\n",
    "\n",
    "# set random seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# probe available devices\n",
    "if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# loop over epochs\n",
    "for epoch in range(epochs):\n",
    "    # initiate losses to 0\n",
    "    train_loss_f = 0\n",
    "    val_loss_f = 0\n",
    "    #loop over all training data\n",
    "    for x in trainloader:\n",
    "        # get data (x[0]) and put the data on the GPU if available\n",
    "        batch=x[0].numpy()\n",
    "        # get the reference f (x[2]) --> note x[1] and x[3] are D and Dp respectively\n",
    "        f_ref = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        # put the data through the neural network\n",
    "        f_pred = model.forward(batch)\n",
    "        # calculate loss (compare predicted f to the ground trueth)\n",
    "        train_loss_f += np.mean(loss_module1.forward(f_pred, f_ref))\n",
    "        # propogate the loss through the network (calculate d_wights/d_loss)\n",
    "        model.backward(loss_module1.backward())\n",
    "\n",
    "        #######################\n",
    "        # PUT YOUR CODE HERE  #\n",
    "        #######################\n",
    "        #updating of the gradients\n",
    "        \n",
    "        for layer in model.LM: \n",
    "            layer.weights -= learning_rate * layer.grads['weight']\n",
    "            layer.bias -= learning_rate * layer.grads['bias']\n",
    "\n",
    "        #######################\n",
    "        # END OF YOUR CODE    #\n",
    "        #######################\n",
    "        \n",
    "    # initialize error_metrics\n",
    "    SD_val=0\n",
    "    sys_val=0\n",
    "    for x in inferloader:\n",
    "        batch=x[0].numpy()\n",
    "        f_ref_val = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        f_pred_val = model.forward(batch)\n",
    "        val_loss_f += np.mean(loss_module1.forward(f_pred_val, f_ref_val))\n",
    "        SD, sys = hf.error_metrics(f_pred_val,f_ref_val)\n",
    "        SD_val += SD**2\n",
    "        sys_val += sys\n",
    "    SD_val = np.sqrt(SD_val/inferloader.__len__())\n",
    "    sys_val = sys_val/inferloader.__len__()\n",
    "\n",
    "    model.clear_cache()\n",
    "    \n",
    "    wandb.log({\"loss/train\": train_loss_f/trainloader.__len__(),\"loss/val\": val_loss_f/inferloader.__len__(),\"error/random error\":SD_val,\"error/systematic error\":sys_val})\n",
    "    print('epoch = ' + str(epoch) + ' train loss =' + str(train_loss_f/trainloader.__len__()) +' val loss =' + str(val_loss_f/inferloader.__len__()) + 'the systematic error is ' + str(sys_val) + ' and the random error is ' + str(SD_val))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch = 0 train loss =0.05000394705769913 val loss =0.04008040137444191the systematic error is -0.0023732136385089097 and the random error is 0.19504413882891897\n",
      "epoch = 1 train loss =0.04104263738717604 val loss =0.04007766593069095the systematic error is -0.0035085891982727427 and the random error is 0.1950210572465974\n",
      "epoch = 2 train loss =0.041006072424439675 val loss =0.04005715380087678the systematic error is 0.0012300790165032931 and the random error is 0.19499722607433415\n",
      "epoch = 3 train loss =0.041009178050058596 val loss =0.0400444057268614the systematic error is -6.106895087787718e-05 and the random error is 0.1949696498783934\n",
      "epoch = 4 train loss =0.04100744717389617 val loss =0.040048754692404494the systematic error is -0.004156552017776363 and the random error is 0.19493793446110974\n",
      "epoch = 5 train loss =0.04097087518779344 val loss =0.040024930067486746the systematic error is 0.0026931107184629097 and the random error is 0.19490405600050603\n",
      "epoch = 6 train loss =0.04093264829880344 val loss =0.04001924288383053the systematic error is -0.00441752932730674 and the random error is 0.1948599956825439\n",
      "epoch = 7 train loss =0.0409280511773613 val loss =0.039996525034092074the systematic error is -0.0041530646526210056 and the random error is 0.19480978890044812\n",
      "epoch = 8 train loss =0.040925514606863896 val loss =0.03996644020960329the systematic error is 0.0032908867519728896 and the random error is 0.19475165803978828\n",
      "epoch = 9 train loss =0.04087352976940266 val loss =0.03997797387458439the systematic error is 0.007257767750526759 and the random error is 0.1946771806777323\n",
      "epoch = 10 train loss =0.04089234627735743 val loss =0.03988307207151023the systematic error is -0.00033051519004380613 and the random error is 0.19457310387095264\n",
      "epoch = 11 train loss =0.040793121458941387 val loss =0.03983770012534196the systematic error is 0.0025735128748458503 and the random error is 0.1944454862987417\n",
      "epoch = 12 train loss =0.04075226692314333 val loss =0.03976767768235715the systematic error is 0.004398092338951244 and the random error is 0.19424176830088719\n",
      "epoch = 13 train loss =0.04065116934125249 val loss =0.039725080593467736the systematic error is -0.009652271217117297 and the random error is 0.19395481981220455\n",
      "epoch = 14 train loss =0.04050501854864029 val loss =0.03949258721344404the systematic error is 0.0016910377809284166 and the random error is 0.1936038255062191\n",
      "epoch = 15 train loss =0.04035986959177083 val loss =0.03928838573738511the systematic error is -0.00531598116687005 and the random error is 0.19303565344221174\n",
      "epoch = 16 train loss =0.04006276617494474 val loss =0.03893388016798827the systematic error is -0.0008778868523639895 and the random error is 0.19222357978555146\n",
      "epoch = 17 train loss =0.03962217888106851 val loss =0.038449312604132715the systematic error is -0.007613486122171523 and the random error is 0.19087062487631837\n",
      "epoch = 18 train loss =0.03895149947007306 val loss =0.03762604599044184the systematic error is -0.01064138884382763 and the random error is 0.18865616446736355\n",
      "epoch = 19 train loss =0.03781207988237133 val loss =0.03610910302004569the systematic error is -0.005644805266373139 and the random error is 0.18498103582581252\n",
      "epoch = 20 train loss =0.035861652499698575 val loss =0.03363175579446762the systematic error is -0.0070734469752729975 and the random error is 0.17841343123905423\n",
      "epoch = 21 train loss =0.032591105938182055 val loss =0.029679103406806375the systematic error is -0.0023967339491284836 and the random error is 0.16761511346178964\n",
      "epoch = 22 train loss =0.027685336367822517 val loss =0.024056780108807532the systematic error is -0.017467703146302176 and the random error is 0.14979099431181975\n",
      "epoch = 23 train loss =0.021718237998202057 val loss =0.01830345147314995the systematic error is -0.015327348488496748 and the random error is 0.13044586009113368\n",
      "epoch = 24 train loss =0.016548542167933132 val loss =0.014219170543631131the systematic error is -0.018878149524000305 and the random error is 0.11408749079205478\n",
      "epoch = 25 train loss =0.013358668924398681 val loss =0.012058865502902824the systematic error is -0.00485956819927486 and the random error is 0.10624332551963112\n",
      "epoch = 26 train loss =0.011799429760761607 val loss =0.011129465475450115the systematic error is -0.005143231302550963 and the random error is 0.10203667327421088\n",
      "epoch = 27 train loss =0.011090347250367406 val loss =0.010711904928847573the systematic error is -0.004924940732494626 and the random error is 0.10011904395738642\n",
      "epoch = 28 train loss =0.010723926058268692 val loss =0.010475561322000932the systematic error is -0.004275284396077698 and the random error is 0.0990451550394616\n",
      "epoch = 29 train loss =0.010469860404802737 val loss =0.01030111962324772the systematic error is -0.00323129951945773 and the random error is 0.09825565085766265\n"
     ]
    }
   ],
   "source": [
    "# define parameters\n",
    "hidden_layers=(64,32)\n",
    "model=MLP(len(bvalues), hidden_layers, 1)\n",
    "epochs = 30\n",
    "learning_rate = 0.001\n",
    "momentum_coeff = 0.9\n",
    "change_weights = {layer: np.zeros_like(layer.weights) for layer in model.LM}\n",
    "change_bias = {layer: np.zeros_like(layer.bias) for layer in model.LM}\n",
    "\n",
    "# initialize wandb\n",
    "wandb.init(\n",
    "        project=\"AI_for_medical_imaging\", job_type=\"training\")\n",
    "\n",
    "loss_module1 = MSE()\n",
    "\n",
    "# set random seed for reproducibility\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# probe available devices\n",
    "if torch.cuda.is_available():  # GPU operation have separate seed\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.determinstic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# loop over epochs\n",
    "for epoch in range(epochs):\n",
    "    # initiate losses to 0\n",
    "    train_loss_f = 0\n",
    "    val_loss_f = 0\n",
    "    #loop over all training data\n",
    "    for x in trainloader:\n",
    "        # get data (x[0]) and put the data on the GPU if available\n",
    "        batch=x[0].numpy()\n",
    "        # get the reference f (x[2]) --> note x[1] and x[3] are D and Dp respectively\n",
    "        f_ref = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        # put the data through the neural network\n",
    "        f_pred = model.forward(batch)\n",
    "        # calculate loss (compare predicted f to the ground trueth)\n",
    "        train_loss_f += np.mean(loss_module1.forward(f_pred, f_ref))\n",
    "        # propogate the loss through the network (calculate d_wights/d_loss)\n",
    "        model.backward(loss_module1.backward())\n",
    "\n",
    "        #######################\n",
    "        # PUT YOUR CODE HERE  #\n",
    "        #######################\n",
    "        #updating of the gradients\n",
    "        for layer in model.LM: \n",
    "\n",
    "            new_change_weights = learning_rate * layer.grads['weight'] + momentum_coeff * change_weights[layer]\n",
    "            new_change_bias = learning_rate * layer.grads['bias'] + momentum_coeff * change_bias[layer]\n",
    "            \n",
    "            layer.weights -= new_change_weights\n",
    "            layer.bias -= new_change_bias\n",
    "\n",
    "            change_weights[layer] = new_change_weights\n",
    "            change_bias[layer] = new_change_bias\n",
    "\n",
    "        #######################\n",
    "        # END OF YOUR CODE    #\n",
    "        #######################\n",
    "        \n",
    "    # initialize error_metrics\n",
    "    SD_val=0\n",
    "    sys_val=0\n",
    "    for x in inferloader:\n",
    "        batch=x[0].numpy()\n",
    "        f_ref_val = np.expand_dims(x[2].numpy(),axis=1)\n",
    "        f_pred_val = model.forward(batch)\n",
    "        val_loss_f += np.mean(loss_module1.forward(f_pred_val, f_ref_val))\n",
    "        SD, sys = hf.error_metrics(f_pred_val,f_ref_val)\n",
    "        SD_val += SD**2\n",
    "        sys_val += sys\n",
    "    SD_val = np.sqrt(SD_val/inferloader.__len__())\n",
    "    sys_val = sys_val/inferloader.__len__()\n",
    "\n",
    "    model.clear_cache()\n",
    "    \n",
    "    wandb.log({\"loss/train\": train_loss_f/trainloader.__len__(),\"loss/val\": val_loss_f/inferloader.__len__(),\"error/random error\":SD_val,\"error/systematic error\":sys_val})\n",
    "    print('epoch = ' + str(epoch) + ' train loss =' + str(train_loss_f/trainloader.__len__()) +' val loss =' + str(val_loss_f/inferloader.__len__()) + 'the systematic error is ' + str(sys_val) + ' and the random error is ' + str(SD_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
